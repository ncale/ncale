---
import { H1 } from "@/components/headings";
import Layout from "@/layouts/Layout.astro";
---

<Layout title="The Nature of Intelligence">
  <H1>The Nature of Intelligence</H1>
  <ul>
    <li>
      The idea of not being the most intelligent thing in the known universe is scary. It makes us vulnerable. The only
      way we can combat that feeling is by neutering the `other`, and dominating it. If it doesn't post a direct threat,
      then we'll ridicule it and diminish it. Think aboout the criticisms of AGI:
    </li>
    <li>
      <small>Parties that benefit from investment in AI are exaggerating development rates</small>
    </li>
    <li>
      <small>Genuine AGI is far off due to the complexity of intelligence</small>
    </li>
    <li>
      <small>...</small>
    </li>
    <li>
      We're going to learn a lot about the nature of intelligence as we develop more intelligent algorithms. It helps to
      understand the current expected course of development from the leading firms:
    </li>
    <li>Agents -> AGI -> ASI</li>
    <li>
      Agents, AGI, and ASI, will have varying capabilities. Agents, the dumbest of the bunch, are really just automated
      processes tuned for a specific task. The first agents that have the tools will be tasked with improving themselves
      to be more efficient, effective, and flexible. In the course of their self-tuning and improvement, agents are then
      expected to either: 1. Improve enough to where they resemble something like an AGI, or 2. Research new strategies
      to cover the gaps we see between the flagship agents and AGI.
    </li>
    <li>
      Number 1 is a very straight forward path requiring very little guesswork and very similar technology than we have
      now. Essentially, we're expecting the AI to implement the expected strategies for improvement that we've already
      identified.
    </li>
    <li>
      Number 2 is a much more interesting path because it requires creativity - of which, the common sentiment is that
      our most intelligent models have exactly none. Here is why I think most AI researchers are confident we'll see AGI
      very soon - whenever a model has to find a way to the solution without being guided, researchers call that
      unsupervised learning, which is essentially the practice of adding randomness to the model, and rewarding it when
      the changes lead to favorable outcomes. This has created very unexpected but outcomes (famous strategy of alpha
      go, etc.).
    </li>
    <li>
      No one really has any idea how soon we'll see AGI, but I think it's a mistake to dismiss it. Humans, for one, are
      not very creative on average. Agents powered by models that surpass the median can already unemploy a dramatic
      amount of people in any information related task.
    </li>

    <li>Agents, which are feasible today, will be researching how to build the first AGI.</li>
    <li>AGI will be researching ASI.</li>

    <li>
      Most people don't take AGI to be a real threat, and I'd like to address some of the more common criticisms with my
      understandings of why they're probably going to be surprised:
    </li>
    <li>
      First: agents aren't creative enough to make the changes needed to achieve AGI. This is true. Agents aren't needed
      to develop ASI though, just AGI. AGI will not be obvious when we achieve it since we don't even have a benchmark
      for human intelligence. How will we really know when we're there? I think we'll only really know once we've passed
      it. Why? The nature of intelligence is complicated. In most ways, AGI will still fall short of people. For one, if
      you have 100 million people and 100 million AGIs, the AGIs are much more likely to be equal in intelligence, where
      the humans are expected to fall on a bell curve. Some of the humans will fall far above the curve, where AGI will
      be consistent. Even if they fall above the average, they will still be considered inferior due to failing to
      surpass in every way.
    </li>
    <li>Second:</li>
    <li>
      Final thoughts - if AGI is on the horizon, most of us that have thought about it would agree that it's a
      terrifying outcome. However, let's think about this practically: given the circumstances, do we really think
      delaying AGI development by 5 years would help our outcome? Rapid, scary development is the only way to wake the
      world up - UBI will be necessary, people will need to find new work or new meaning, and working together is more
      important than beating eachother.
    </li>
  </ul>
</Layout>
