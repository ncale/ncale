---
layout: "../../layouts/page-layout.astro"
title: "The Nature of Intelligence"
description: "The idea of not being the most intelligent thing in the known universe is scary. It makes us vulnerable. The only way we can combat that feeling is by neutering the `other`, and dominating it. If it doesn't post a direct threat, then we'll ridicule it and diminish it."
author: "Nick Brodeur"
dateStarted: "" # format: YYYY-MM-DD
lastUpdated: "" # format: YYYY-MM-DD
image: {
	src: "",
	alt: ""
}
category: ""
draft: true
---

The idea of not being the most intelligent thing in the known universe is scary. It makes us vulnerable. The only
way we can combat that feeling is by neutering the `other`, and dominating it. If it doesn't post a direct threat,
then we'll ridicule it and diminish it. Think aboout the criticisms of AGI:

<small>Parties that benefit from investment in AI are exaggerating development rates</small>
<small>Genuine AGI is far off due to the complexity of intelligence</small>
<small>...</small>

We're going to learn a lot about the nature of intelligence as we develop more intelligent algorithms. It helps to
understand the current expected course of development from the leading firms:

## Agents -> AGI -> ASI

Agents, AGI, and ASI, will have varying capabilities. Agents, the dumbest of the bunch, are really just automated
processes tuned for a specific task. The first agents that have the tools will be tasked with improving themselves
to be more efficient, effective, and flexible. In the course of their self-tuning and improvement, agents are then
expected to either: 1. Improve enough to where they resemble something like an AGI, or 2. Research new strategies
to cover the gaps we see between the flagship agents and AGI.

Number 1 is a very straight forward path requiring very little guesswork and very similar technology than we have
now. Essentially, we're expecting the AI to implement the expected strategies for improvement that we've already
identified.

Number 2 is a much more interesting path because it requires creativity - of which, the common sentiment is that
our most intelligent models have exactly none. Here is why I think most AI researchers are confident we'll see AGI
very soon - whenever a model has to find a way to the solution without being guided, researchers call that
unsupervised learning, which is essentially the practice of adding randomness to the model, and rewarding it when
the changes lead to favorable outcomes. This has created very unexpected but outcomes (famous strategy of alpha
go, etc.).

No one really has any idea how soon we'll see AGI, but I think it's a mistake to dismiss it. Humans, for one, are
not very creative on average. Agents powered by models that surpass the median can already unemploy a dramatic
amount of people in any information related task.

Agents, which are feasible today, will be researching how to build the first AGI.
AGI will be researching ASI.

Most people don't take AGI to be a real threat, and I'd like to address some of the more common criticisms with my
understandings of why they're probably going to be surprised:

First: agents aren't creative enough to make the changes needed to achieve AGI. This is true. Agents aren't needed
to develop ASI though, just AGI. AGI will not be obvious when we achieve it since we don't even have a benchmark
for human intelligence. How will we really know when we're there? I think we'll only really know once we've passed
it. Why? The nature of intelligence is complicated. In most ways, AGI will still fall short of people. For one, if
you have 100 million people and 100 million AGIs, the AGIs are much more likely to be equal in intelligence, where
the humans are expected to fall on a bell curve. Some of the humans will fall far above the curve, where AGI will
be consistent. Even if they fall above the average, they will still be considered inferior due to failing to
surpass in every way.

Second:

Final thoughts - if AGI is on the horizon, most of us that have thought about it would agree that it's a
terrifying outcome. However, let's think about this practically: given the circumstances, do we really think
delaying AGI development by 5 years would help our outcome? Rapid, scary development is the only way to wake the
world up - UBI will be necessary, people will need to find new work or new meaning, and working together is more
important than beating eachother.
